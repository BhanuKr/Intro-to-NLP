{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Af-tIG-B2yzS"
      },
      "outputs": [],
      "source": [
        "STUDENT_SAP_NAME  = \"Bhanu Kumar\"\n",
        "STUDENT_SR_NUMBER = \"22226\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "import nltk\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader as api\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKDPuBGd2-dh",
        "outputId": "964c35be-be88-4d58-d7fa-c9902183760f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset(url, output_path):\n",
        "    \"\"\"\n",
        "    Download a CSV file from a given URL and save it to the specified path.\n",
        "    If the file already exists, skip the download.\n",
        "\n",
        "    Parameters:\n",
        "    url (str): URL of the CSV file to download\n",
        "    output_path (str): Path where the file should be saved\n",
        "\n",
        "    Returns:\n",
        "    bool: True if download was successful or file already exists, False otherwise\n",
        "    \"\"\"\n",
        "    # Convert to Path object for easier path manipulation\n",
        "    output_path = Path(output_path)\n",
        "\n",
        "    # Check if file already exists\n",
        "    if output_path.exists():\n",
        "        print(f\"File already exists: {output_path}\")\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        print(f\"Downloading {output_path.name}...\")\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        # Create parent directories if they don't exist\n",
        "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        with open(output_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Successfully downloaded: {output_path}\")\n",
        "        return True\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading {output_path}: {e}\")\n",
        "        return False\n",
        "    except IOError as e:\n",
        "        print(f\"Error saving file {output_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "# URLs for the datasets\n",
        "urls = {\n",
        "    'train': \"https://docs.google.com/spreadsheets/d/1pHK8joOen4R1KlhF5Re3LZFb4Dt0n4jraHoTGMBgtF4/export?format=csv\",\n",
        "    'val': \"https://docs.google.com/spreadsheets/d/1t2J2EJPo-P2AlDOAybxq7nX61mbZwgMoQpR_J3FneJ4/export?format=csv\",\n",
        "}\n",
        "\n",
        "# Download all datasets\n",
        "for dataset_type, url in urls.items():\n",
        "    output_path = f\"downloaded_datasets/{dataset_type}_data.csv\"\n",
        "    download_dataset(url, output_path)\n",
        "\n",
        "df = pd.read_csv('downloaded_datasets/train_data.csv')\n",
        "df_val = pd.read_csv('downloaded_datasets/val_data.csv')\n",
        "\n",
        "df.head()\n",
        "\n",
        "sentiment_percentages = df_val['sentiment'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"Sentiment Distribution:\")\n",
        "print(f\"Class 0: {sentiment_percentages[0]:.2f}%\")\n",
        "print(f\"Class 1: {sentiment_percentages[1]:.2f}%\")\n",
        "\n",
        "X_train, y_train = df.review.values.tolist(), df.sentiment.values.tolist()\n",
        "X_val, y_val = df_val.review.values.tolist(), df_val.sentiment.values.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-drfstSw3CT8",
        "outputId": "2adb35fc-f004-40be-908d-dd8fb1b3a27e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading train_data.csv...\n",
            "Successfully downloaded: downloaded_datasets/train_data.csv\n",
            "Downloading val_data.csv...\n",
            "Successfully downloaded: downloaded_datasets/val_data.csv\n",
            "Sentiment Distribution:\n",
            "Class 0: 49.95%\n",
            "Class 1: 50.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classifier(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate classification accuracy.\n",
        "\n",
        "    Args:\n",
        "        y_true (list): True class labels\n",
        "            Example: [0, 1, 0, 1]\n",
        "        y_pred (list): Predicted class labels\n",
        "            Example: [0, 1, 1, 1]\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy (proportion of correct predictions)\n",
        "            Example: 0.75 (3 correct predictions out of 4)\n",
        "\n",
        "    Note:\n",
        "        - Accuracy = (number of correct predictions) / (total number of predictions)\n",
        "        - Raises ValueError if lengths of inputs don't match\n",
        "    \"\"\"\n",
        "    if len(y_true) != len(y_pred):\n",
        "        raise ValueError(\"Length of true and predicted labels must match\")\n",
        "\n",
        "    correct = sum(1 for t, p in zip(y_true, y_pred) if t == p)\n",
        "    return correct / len(y_true)\n",
        "\n",
        "\"\"\"# Generative Classification\n",
        "\n",
        "## Naive Bayes Text Classification\n",
        "\n",
        " This implementation covers a Naive Bayes classifier for text classification.\n",
        " The key mathematical foundation is Bayes' theorem:\n",
        "\n",
        " P(class|document) ∝ P(class) * P(document|class)\n",
        "\n",
        " Where:\n",
        " - P(class|document) is the posterior probability\n",
        " - P(class) is the prior probability of the class\n",
        " - P(document|class) is the likelihood of the document given the class\n",
        "\n",
        " Under the \"naive\" assumption of conditional independence:\n",
        " P(document|class) = P(word1|class) * P(word2|class) * ... * P(wordN|class)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self, min_freq=1):\n",
        "        \"\"\"\n",
        "        Initialize the Naive Bayes classifier.\n",
        "\n",
        "        Args:\n",
        "            min_freq (int): Minimum frequency threshold for a word to be included in vocabulary.\n",
        "                           Words appearing less than min_freq times will be treated as UNK token.\n",
        "                           Default: 1 (include all words)\n",
        "\n",
        "        Attributes:\n",
        "            class_probs (dict): P(class) for each class\n",
        "                Example: {0: 0.5, 1: 0.5}\n",
        "\n",
        "            word_probs (dict): P(word|class) for each word and class\n",
        "                Example: {\n",
        "                    0: {'hello': 0.5, 'world': 0.4, '<UNK>': 0.1},\n",
        "                    1: {'hello': 0.3, 'world': 0.5, '<UNK>': 0.2}\n",
        "                }\n",
        "\n",
        "            vocabulary (dict): Word to index mapping, including special UNK token\n",
        "                Example: {'<UNK>': 0, 'hello': 1, 'world': 2}\n",
        "\n",
        "            min_freq (int): Minimum frequency threshold for vocabulary inclusion\n",
        "                Example: If min_freq=2, words must appear at least twice to be included\n",
        "\n",
        "        Note:\n",
        "            - Words appearing less than min_freq times in training data will be mapped to <UNK>\n",
        "            - <UNK> token is automatically added to vocabulary as first token (index 0)\n",
        "            - Probability for <UNK> is calculated during training based on rare words\n",
        "        \"\"\"\n",
        "        self.class_probs = None\n",
        "        self.word_probs = None\n",
        "        self.vocabulary = None\n",
        "        self.min_freq = min_freq\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocess the input text by converting to lowercase, removing non-word characters,\n",
        "        and filtering out common stop words.\n",
        "\n",
        "        Args:\n",
        "            text (str): Raw input text\n",
        "                Example: \"Hello, World! How are you doing today?\"\n",
        "\n",
        "        Returns:\n",
        "            list: List of cleaned, tokenized, and filtered words with stop words removed\n",
        "                Example: ['hello', 'world', 'doing', 'today']\n",
        "\n",
        "        Note:\n",
        "            - Converts all text to lowercase\n",
        "            - Removes punctuation and special characters\n",
        "            - Splits text into individual tokens\n",
        "            - Removes common English stop words (e.g., 'a', 'an', 'the', 'is', 'are', 'how')\n",
        "            - Stop words are removed using NLTK's English stop words list\n",
        "        \"\"\"\n",
        "        # Import stop words from NLTK\n",
        "        from nltk.corpus import stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Extract word characters only and split into tokens\n",
        "        tokens = re.findall(r'\\w+', text)\n",
        "\n",
        "        # Remove stop words\n",
        "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "        return filtered_tokens\n",
        "\n",
        "    def create_vocabulary(self, texts):\n",
        "        \"\"\"\n",
        "        Create vocabulary from training texts by mapping unique words to indices,\n",
        "        considering minimum frequency threshold and adding UNK token.\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text documents\n",
        "                Example: [\n",
        "                    \"Hello world hello\",\n",
        "                    \"Hello there\",\n",
        "                    \"World is beautiful\"\n",
        "                ]\n",
        "\n",
        "        Returns:\n",
        "            dict: Mapping of words to unique indices, including UNK token\n",
        "                Example (with min_freq=2): {\n",
        "                    '<UNK>': 0,    # Special token for rare/unseen words\n",
        "                    'hello': 1,    # Frequency=3, included in vocab\n",
        "                    'world': 2,    # Frequency=2, included in vocab\n",
        "                    # 'there' and 'beautiful' not included (frequency=1 < min_freq=2)\n",
        "                }\n",
        "\n",
        "        Note:\n",
        "            - Always includes <UNK> token at index 0\n",
        "            - Only includes words that appear >= min_freq times\n",
        "            - Word frequency is counted across all documents\n",
        "            - Uses preprocess_text function for preprocessing\n",
        "            - Words below frequency threshold will be mapped to UNK during feature extraction\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        tokenized_list = [] # list of list of tokenized preprocessed words for each text .\n",
        "        for text in texts:\n",
        "            tokens = self.preprocess_text(text)\n",
        "            tokenized_list.append(tokens)\n",
        "\n",
        "        word_freq = {} # dictionary to store frequency of each word .\n",
        "        for text in tokenized_list:\n",
        "            for token in text:\n",
        "                if token in word_freq:\n",
        "                    word_freq[token] += 1\n",
        "                else:\n",
        "                    word_freq[token] = 1\n",
        "\n",
        "        vocabulary = {'<UNK>': 0}\n",
        "        index = 1\n",
        "        for word in word_freq.keys():\n",
        "            if word_freq[word] >= self.min_freq: # if frequency of word is greater than or equal to min_freq then add it to vocabulary .\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "        return vocabulary\n",
        "\n",
        "\n",
        "\n",
        "    def extract_features(self, texts, vocabulary):\n",
        "        \"\"\"\n",
        "        Convert texts to bag-of-words feature vectors using the vocabulary,\n",
        "        where each element represents the count of word occurrences (not binary presence/absence).\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text documents\n",
        "                Example: [\"hello world hello\", \"world is beautiful\"]\n",
        "            vocabulary (dict): Word to index mapping with UNK token\n",
        "                Example: {'<UNK>': 0, 'hello': 1, 'world': 2}\n",
        "\n",
        "        Returns:\n",
        "            np.array: Feature matrix where each row is a document vector\n",
        "                Example: For the above input with min_freq=2:\n",
        "                array([\n",
        "                    [0, 2, 1],  # First doc: 0 UNKs, 2 'hello's, 1 'world'\n",
        "                    [2, 0, 1]   # Second doc: 2 UNKs (one each for 'is' and 'beautiful'), 0 'hello's, 1 'world',\n",
        "                ])\n",
        "\n",
        "        Note:\n",
        "            - Each row represents one document\n",
        "            - Each column represents the count of a specific word\n",
        "            - First column is always UNK token count\n",
        "            - Words not in vocabulary are counted as UNK\n",
        "            - Shape of output: (n_documents, len(vocabulary))\n",
        "            - Uses preprocess_text function for preprocessing\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        preprocessed_texts = [self.preprocess_text(text) for text in texts] # preprocess each text in texts .\n",
        "\n",
        "        list_ = []\n",
        "        for text in preprocessed_texts:\n",
        "            list = [0 for i in range(len(vocabulary))]\n",
        "            for token in text:\n",
        "                if token in vocabulary:\n",
        "                    list[vocabulary[token]] += 1# if token is in vocabulary then increment the count of that token in list .\n",
        "                else:\n",
        "                    list[0] += 1# if token is not in vocabulary then increment the count of UNK token in list .\n",
        "\n",
        "            list_.append(list)\n",
        "\n",
        "        return np.array(list_)\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_class_probabilities(self, y):\n",
        "        \"\"\"\n",
        "        Estimate probability P(class) for each class from training labels.\n",
        "\n",
        "        Args:\n",
        "            y (list): List of class labels\n",
        "                Example: [0, 0, 1, 1, 0, 1]\n",
        "\n",
        "        Returns:\n",
        "            dict: Estimated probability for each class\n",
        "                Example: {\n",
        "                    0: 0.5,    # 3 out of 6 samples are class 0\n",
        "                    1: 0.5     # 3 out of 6 samples are class 1\n",
        "                }\n",
        "\n",
        "        Note:\n",
        "            - Probabilities sum to 1 across all classes\n",
        "            - Handles any number of unique classes\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        y = pd.Series(y) # convert list to pandas series .\n",
        "        class_probabilities = y.value_counts(normalize=True) # calculate probability of each class .\n",
        "        class_probabilities = class_probabilities.to_dict() # convert pandas series to dictionary .\n",
        "\n",
        "        return class_probabilities\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_word_probabilities(self, X, y, vocabulary, alpha=1.0):\n",
        "        \"\"\"\n",
        "        Calculate conditional probability P(word|class) for each word and class,\n",
        "        including probability for UNK token.\n",
        "\n",
        "        Args:\n",
        "            X (np.array): Document-term matrix (with UNK counts in first column)\n",
        "                Example: array([\n",
        "                    [0, 2, 1],  # Document 1: 0 UNKs, 2 of word 1, 1 of word 2\n",
        "                    [1, 0, 1],  # Document 2: 1 UNK, 0 of word 1, 1 of word 2\n",
        "                ])\n",
        "            y (list): Class labels\n",
        "                Example: [0, 1]\n",
        "            vocabulary (dict): Word to index mapping with UNK token\n",
        "                Example: {'<UNK>': 0, 'hello': 1, 'world': 2}\n",
        "            alpha (float): Laplace smoothing parameter, default=1.0\n",
        "\n",
        "        Returns:\n",
        "            dict: Nested dict with P(word|class) for each word and class\n",
        "                Example: {\n",
        "                    0: {\n",
        "                        '<UNK>': 0.167,    # P(word=UNK|class=0)\n",
        "                        'hello': 0.5,     # P(word='hello'|class=0)\n",
        "                        'world': 0.333      # P(word='world'|class=0)\n",
        "                    },\n",
        "                    1: {\n",
        "                        '<UNK>': 0.4,    # P(word=UNK|class=1)\n",
        "                        'hello': 0.2,     # P(word='hello'|class=1)\n",
        "                        'world': 0.4      # P(word='world'|class=1)\n",
        "                    }\n",
        "                }\n",
        "\n",
        "        Note:\n",
        "            - Uses Laplace smoothing to handle unseen words\n",
        "            - UNK token probability is learned from training data\n",
        "            - Formula: P(word|class) = (count(word,class) + α) / (total_words_in_class + α|V|)\n",
        "            - |V| is vocabulary size (including UNK token)\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        k = 2 # number of classes .\n",
        "\n",
        "        class_dict = {i: [] for i in range(k)} #Dictionary to store the list of the extracted features of the texts belonging to the respective class.\n",
        "        for i in range(len(y)):\n",
        "            class_dict[y[i]].append(X[i])\n",
        "\n",
        "        total_words_in_class = {} #Dictionary to store the total number of words in each class.\n",
        "        for i in range(k):\n",
        "            total_words_in_class[i] = sum([sum(class_dict[i][j]) for j in range(len(class_dict[i]))])\n",
        "\n",
        "        word_count = {} #Dictionary to store the count of each word in each class.\n",
        "        for i in range(k):\n",
        "            word_count[i] = {}\n",
        "            for j in range(len(vocabulary)):\n",
        "                word_count[i][j] = sum([class_dict[i][l][j] for l in range(len(class_dict[i]))])\n",
        "\n",
        "        word_probs = {}#Dictionary to store the probability of each word in each class, using the formula .\n",
        "        for i in range(k):\n",
        "            word_probs[i] = {}\n",
        "            for j in range(len(vocabulary)):\n",
        "                word_probs[i][list(vocabulary.keys())[j]] = (word_count[i][j] + alpha) / (total_words_in_class[i] + alpha * len(vocabulary))\n",
        "\n",
        "        return word_probs\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, X_text, y):\n",
        "        \"\"\"\n",
        "        Train the Naive Bayes classifier on the provided text documents.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\n",
        "                    \"hello world\",\n",
        "                    \"beautiful world\",\n",
        "                    \"hello there\"\n",
        "                ]\n",
        "            y (list): Class labels\n",
        "                Example: [0, 1, 0]\n",
        "\n",
        "        Note:\n",
        "            - Creates vocabulary from training texts\n",
        "            - Calculates prior probabilities P(class)\n",
        "            - Calculates conditional probabilities P(word|class)\n",
        "            - Stores all necessary parameters for prediction\n",
        "        \"\"\"\n",
        "        # Create vocabulary from training texts\n",
        "        self.vocabulary = self.create_vocabulary(X_text)\n",
        "\n",
        "        # Convert texts to feature vectors\n",
        "        X = self.extract_features(X_text, self.vocabulary)\n",
        "\n",
        "        # Calculate probabilities\n",
        "        self.class_probs = self.calculate_class_probabilities(y)\n",
        "        self.word_probs = self.calculate_word_probabilities(\n",
        "            X, y, self.vocabulary)\n",
        "\n",
        "    def predict(self, X_text):\n",
        "        \"\"\"\n",
        "        Predict classes for new documents using Naive Bayes algorithm,\n",
        "        handling unknown words using UNK token.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\n",
        "                    \"hello world\",\n",
        "                    \"beautiful day\"  # 'day' is unknown, treated as UNK\n",
        "                ]\n",
        "\n",
        "        Returns:\n",
        "            list: Predicted class labels\n",
        "                Example: [0, 1]\n",
        "\n",
        "        Theory:\n",
        "            The standard Naive Bayes formula for text classification is:\n",
        "            P(class|document) ∝ P(class) * ∏ P(word|class)\n",
        "\n",
        "            For unknown words not in vocabulary:\n",
        "            - They are mapped to UNK token\n",
        "            - P(UNK|class) is used in probability calculation\n",
        "\n",
        "            We use log space to prevent numerical underflow:\n",
        "            log(P(class|document)) ∝ log(P(class)) + Σ log(P(word|class))\n",
        "\n",
        "        Implementation:\n",
        "            For each document:\n",
        "            1. Preprocess and tokenize text\n",
        "            2. Replace unknown words with UNK token\n",
        "            3. Calculate log probabilities using appropriate word or UNK probabilities\n",
        "            4. Return class with highest log probability score\n",
        "\n",
        "        Note:\n",
        "            - Uses preprocess_text function for preprocessing\n",
        "            - Words not in vocabulary are treated as UNK token\n",
        "            - UNK probability is used for out-of-vocabulary words\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        extracted_features = np.array(self.extract_features(X_text, self.vocabulary))#extract features of the texts in X_text .\n",
        "\n",
        "        class_probs_array = np.array([self.class_probs[c] for c in sorted(self.class_probs.keys())])# Convert dictionary-based word probabilities to a NumPy array for faster matrix calculations later .\n",
        "        word_probs_array = np.array([\n",
        "            [self.word_probs[c][word] for word in self.vocabulary] for c in sorted(self.class_probs.keys())\n",
        "        ])  # Convert dictionary-based word probabilities to a NumPy array for faster matrix calculations later .\n",
        "\n",
        "        # Log probabilities of classes and words ndarrays .\n",
        "        log_class_probs = np.log(class_probs_array)\n",
        "        log_word_probs = np.log(word_probs_array)\n",
        "\n",
        "        # Compute log probabilities for each text .\n",
        "        log_probs = np.dot(extracted_features, log_word_probs.T) + log_class_probs\n",
        "\n",
        "        # Get class with highest probability for each text\n",
        "        return np.argmax(log_probs, axis=1).tolist()\n",
        "\n",
        "\n",
        "\n",
        "    def get_important_words(self, n=5, use_ratio=True):\n",
        "        \"\"\"\n",
        "        Get the most important words for each class based either on their raw conditional\n",
        "        probabilities or their probability ratios between classes.\n",
        "\n",
        "        Args:\n",
        "            n (int): Number of top words to return for each class, default=5\n",
        "            use_ratio (bool): If True, ranks words by probability ratio between classes\n",
        "                            If False, ranks words by raw conditional probability\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary mapping class labels to lists of (word, score) tuples,\n",
        "                  where score is either probability or probability ratio\n",
        "                Example with use_ratio=False: {\n",
        "                    0: [('excellent', 0.014), ('great', 0.012), ('amazing', 0.011),\n",
        "                        ('<UNK>', 0.008), ('wonderful', 0.007)],  # Raw probabilities\n",
        "                    1: [('terrible', 0.015), ('bad', 0.012), ('<UNK>', 0.010),\n",
        "                        ('boring', 0.008), ('awful', 0.007)]\n",
        "                }\n",
        "                Example with use_ratio=True: {\n",
        "                    0: [('excellent', 7.5), ('amazing', 6.2), ('great', 5.8),\n",
        "                        ('wonderful', 4.9), ('good', 4.1)],  # P(word|pos)/P(word|neg)\n",
        "                    1: [('terrible', 8.3), ('awful', 7.1), ('bad', 6.4),\n",
        "                        ('boring', 5.2), ('waste', 4.8)]    # P(word|neg)/P(word|pos)\n",
        "                }\n",
        "\n",
        "        Note:\n",
        "            - When use_ratio=True:\n",
        "                - For class 0: Returns words where P(word|class=0)/P(word|class=1) is highest\n",
        "                - For class 1: Returns words where P(word|class=1)/P(word|class=0) is highest\n",
        "                - Better at finding discriminative words that distinguish between classes\n",
        "                - Reduces overlap between top words of different classes\n",
        "            - When use_ratio=False:\n",
        "                - Returns words with highest raw P(word|class) for each class\n",
        "                - May have significant overlap between classes\n",
        "            - Includes UNK token only if it meets the ranking criteria\n",
        "            - Small probabilities are handled safely to avoid division by zero\n",
        "        \"\"\"\n",
        "        if not self.word_probs:\n",
        "            raise ValueError(\"Classifier must be trained before getting important words\")\n",
        "\n",
        "        important_words = {}\n",
        "        classes = sorted(self.word_probs.keys())  # Get classes in consistent order\n",
        "\n",
        "        for cls in classes:\n",
        "            other_cls = [c for c in classes if c != cls][0]  # Get the other class\n",
        "\n",
        "            if use_ratio:\n",
        "                # Calculate probability ratios for all words\n",
        "                word_scores = []\n",
        "                for word in self.vocabulary:\n",
        "                    # Add small epsilon to denominator to avoid division by zero\n",
        "                    ratio = (self.word_probs[cls][word] /\n",
        "                            (self.word_probs[other_cls][word] + 1e-4))\n",
        "                    word_scores.append((word, ratio))\n",
        "            else:\n",
        "                # Use raw probabilities\n",
        "                word_scores = list(self.word_probs[cls].items())\n",
        "\n",
        "            # Sort by score (either ratio or probability) and take top n\n",
        "            sorted_words = sorted(word_scores, key=lambda x: x[1], reverse=True)\n",
        "            important_words[cls] = sorted_words[:n]\n",
        "\n",
        "        return important_words\n"
      ],
      "metadata": {
        "id": "8hIiALBj3VBc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_naive_bayes_example():\n",
        "    \"\"\"\n",
        "    Example demonstrating how to use the NaiveBayesClassifier.\n",
        "    \"\"\"\n",
        "    # Sample training data\n",
        "    X_example_train = [\n",
        "        \"I love this movie\",\n",
        "        \"Great film, amazing actors\",\n",
        "        \"Terrible waste of time\",\n",
        "        \"Poor acting, bad script\",\n",
        "        \"Excellent movie, highly recommend\"\n",
        "    ]\n",
        "    y_example_train = [1, 1, 0, 0, 1]  # 1: positive, 0: negative\n",
        "\n",
        "    # Sample validation data\n",
        "    X_example_val = [\n",
        "        \"Really enjoyed this film\",\n",
        "        \"Waste of money, terrible\"\n",
        "    ]\n",
        "    y_example_val = [1, 0]\n",
        "\n",
        "    # Train classifier\n",
        "    nb_classifier = NaiveBayesClassifier(min_freq=1)\n",
        "    nb_classifier.fit(X_example_train, y_example_train)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = nb_classifier.predict(X_example_val)\n",
        "\n",
        "    # Evaluate\n",
        "    accuracy = evaluate_classifier(y_example_val, predictions)\n",
        "    print(f\"Validation accuracy on this example dataset: {accuracy:.4f}\")\n",
        "\n",
        "    # Get and print important words\n",
        "    important_words = nb_classifier.get_important_words(n=5)\n",
        "    for class_label, words in important_words.items():\n",
        "        sentiment = \"Negative\" if class_label == 0 else \"Positive\"\n",
        "        print(f\"\\nTop words for {sentiment} sentiment:\")\n",
        "        for word, prob in words:\n",
        "            print(f\"{word}: {prob:.4f}\")\n",
        "\n",
        "train_and_evaluate_naive_bayes_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiaLzkYG3gOD",
        "outputId": "3f5242c0-1520-4592-cd79-e33b99a33a6a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy on this example dataset: 1.0000\n",
            "\n",
            "Top words for Negative sentiment:\n",
            "terrible: 2.2439\n",
            "waste: 2.2439\n",
            "time: 2.2439\n",
            "poor: 2.2439\n",
            "acting: 2.2439\n",
            "\n",
            "Top words for Positive sentiment:\n",
            "movie: 2.6603\n",
            "love: 1.7735\n",
            "great: 1.7735\n",
            "film: 1.7735\n",
            "amazing: 1.7735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_naive_bayes_main():\n",
        "    \"\"\"\n",
        "    Train and evaluate the Naive Bayes classifier.\n",
        "    \"\"\"\n",
        "    nb_classifier = NaiveBayesClassifier(min_freq=3)\n",
        "    nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "    predictions = nb_classifier.predict(X_val)\n",
        "    accuracy = evaluate_classifier(y_val, predictions)\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Get and print important words\n",
        "    important_words = nb_classifier.get_important_words(n=10)\n",
        "    for class_label, words in important_words.items():\n",
        "        sentiment = \"Negative\" if class_label == 0 else \"Positive\"\n",
        "        print(f\"\\nTop words for {sentiment} sentiment:\")\n",
        "        for word, prob in words:\n",
        "            print(f\"{word}: {prob:.4f}\")\n",
        "\n",
        "# Above 80% validation accuracy is good!\n",
        "train_and_evaluate_naive_bayes_main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wt8bp2B3mFN",
        "outputId": "a6ea5eb7-1860-4e8f-efec-617fd141e7bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8433\n",
            "\n",
            "Top words for Negative sentiment:\n",
            "worst: 6.1628\n",
            "awful: 5.3853\n",
            "waste: 5.0829\n",
            "stupid: 4.2850\n",
            "bad: 3.9889\n",
            "terrible: 3.8646\n",
            "horrible: 3.8016\n",
            "worse: 3.0935\n",
            "crap: 3.0789\n",
            "poor: 2.9945\n",
            "\n",
            "Top words for Positive sentiment:\n",
            "amazing: 2.9914\n",
            "excellent: 2.7799\n",
            "wonderful: 2.7467\n",
            "fantastic: 2.6776\n",
            "loved: 2.5074\n",
            "great: 2.2642\n",
            "favorite: 2.2347\n",
            "best: 2.1258\n",
            "today: 2.1067\n",
            "superb: 2.0682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Word2Vec and Word Analogies: Understanding Semantic Relationships\n",
        "\n",
        " Word embeddings have revolutionized NLP by capturing semantic relationships between words in dense vector spaces. Word2Vec, introduced by Mikolov et al. (2013), maps words to continuous vector representations where similar words cluster together and relationships between words are preserved as vector operations.\n",
        "\n",
        "## Key concepts\n",
        " - Words are represented as dense vectors in high-dimensional space (typically 300D)\n",
        " - Similar words have similar vector representations\n",
        " - Vector arithmetic captures semantic relationships\n",
        " - Famous example: king - man + woman ≈ queen\n",
        "\n",
        " This notebook explores implementation and evaluation of word analogies using different similarity metrics.\n",
        "\n",
        "## Download word2vec\n",
        "\"\"\"\n",
        "\n",
        "def download_word2vec_model(model_name=\"word2vec-google-news-300\"):\n",
        "    \"\"\"\n",
        "    Download word2vec model using gensim's built-in downloader.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Name of the model to download.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the downloaded model\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the specified model is not available\n",
        "        Exception: For other download or processing errors\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if model is available\n",
        "        available_models = api.info()['models'].keys()\n",
        "        if model_name not in available_models:\n",
        "            raise ValueError(\n",
        "                f\"Model '{model_name}' not found. Available models: {', '.join(available_models)}\"\n",
        "            )\n",
        "\n",
        "        print(f\"Downloading {model_name}...\")\n",
        "        model_path = api.load(model_name, return_path=True)\n",
        "        print(f\"Model downloaded successfully to: {model_path}\")\n",
        "\n",
        "        return model_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading model: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "word2vec_path = download_word2vec_model()\n",
        "\n",
        "\"\"\" ## Vector Operations and Similarity Metrics\n",
        "\n",
        " We implement two key similarity metrics:\n",
        " 1. Cosine Similarity: Measures angle between vectors, normalized to [-1,1]\n",
        " 2. Euclidean Similarity: Based on straight-line distance between vectors\n",
        "\n",
        " The class below handles vector operations and similarity computations.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class WordEmbeddingOps:\n",
        "    def __init__(self, word2vec_path):\n",
        "        \"\"\"\n",
        "        Initialize the WordEmbeddings class with a pre-trained word2vec model.\n",
        "\n",
        "        Args:\n",
        "            word2vec_path (str): Path to the word2vec model file\n",
        "                Example: 'path/to/GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "        Note:\n",
        "            - Loads word vectors using gensim's KeyedVectors\n",
        "        \"\"\"\n",
        "        self.word_vectors = KeyedVectors.load_word2vec_format(\n",
        "            word2vec_path, binary=True)\n",
        "\n",
        "    def cosine_similarity(self, vec1, vec2):\n",
        "        \"\"\"\n",
        "        Calculate cosine similarity between two vectors.\n",
        "\n",
        "        Args:\n",
        "            vec1 (np.array): First vector\n",
        "                Example: array([0.2, 0.5, -0.1])\n",
        "            vec2 (np.array): Second vector\n",
        "                Example: array([0.3, 0.4, -0.2])\n",
        "\n",
        "        Returns:\n",
        "            float: Cosine similarity between vectors\n",
        "                Example: 0.95 (for above vectors)\n",
        "\n",
        "        Note:\n",
        "            - Cosine similarity = vec1 · vec2 / (||vec1|| ||vec2||)\n",
        "            - Range: [-1, 1], where 1 means same direction\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # dot product of the two vectors\n",
        "        dot_product = np.dot(vec1, vec2)\n",
        "\n",
        "        # magnitude of the two vectors\n",
        "        magnitude_vec1 = np.linalg.norm(vec1)\n",
        "        magnitude_vec2 = np.linalg.norm(vec2)\n",
        "\n",
        "        # cosine similarity\n",
        "        cosine_similarity = dot_product / (magnitude_vec1 * magnitude_vec2)\n",
        "        return cosine_similarity\n",
        "\n",
        "\n",
        "\n",
        "    def euclidean_similarity(self, vec1, vec2):\n",
        "        \"\"\"\n",
        "        Calculate similarity based on Euclidean distance.\n",
        "\n",
        "        Args:\n",
        "            vec1 (np.array): First vector\n",
        "                Example: array([0.2, 0.5, -0.1])\n",
        "            vec2 (np.array): Second vector\n",
        "                Example: array([0.3, 0.4, -0.2])\n",
        "\n",
        "        Returns:\n",
        "            float: Similarity score based on Euclidean distance\n",
        "                Example: 0.85\n",
        "\n",
        "        Note:\n",
        "            - Converts Euclidean distance to similarity\n",
        "            - similarity = 1 / (1 + distance)\n",
        "            - Range: (0, 1], where 1 means identical vectors\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # euclidean distance between the two vectors\n",
        "        euclidean_distance = np.linalg.norm(vec1 - vec2)\n",
        "\n",
        "        # similarity based on euclidean distance\n",
        "        euclidean_similarity = 1 / (1 + euclidean_distance)\n",
        "\n",
        "        return euclidean_similarity\n",
        "\n",
        "\n",
        "\n",
        "    def find_analogies(self, word1, word2, word3, similarity_func='cosine', num_results=5):\n",
        "        \"\"\"\n",
        "        Find the words that complete the analogy: word1 : word2 :: word3 : ?\n",
        "\n",
        "        Args:\n",
        "            word1 (str): First word in the analogy\n",
        "                Example: 'king'\n",
        "            word2 (str): Second word in the analogy\n",
        "                Example: 'man'\n",
        "            word3 (str): Third word in the analogy\n",
        "                Example: 'queen'\n",
        "            num_results (int): Number of top results to return\n",
        "                Example: 5\n",
        "            similarity_func (str): Similarity function to use ('cosine' or 'euclidean')\n",
        "\n",
        "        Returns:\n",
        "            list: List of tuples (word, similarity_score) for top num_results matches\n",
        "                Example: [('woman', 0.95), ('girl', 0.82), ('lady', 0.78), ...]\n",
        "\n",
        "        Note:\n",
        "            - Uses vector arithmetic: word2 - word1 + word3\n",
        "            - Excludes input words from results\n",
        "            - Returns empty list if any input word not in vocabulary\n",
        "            - Implementation iterates through all words in vocabulary using:\n",
        "              for word in self.word_vectors.index_to_key\n",
        "              This is necessary to compare the target vector against every\n",
        "              possible word in the model's vocabulary\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # If any of the input words are not in the vocabulary , return an empty list.\n",
        "        if word1 not in self.word_vectors or word2 not in self.word_vectors or word3 not in self.word_vectors:\n",
        "            return []\n",
        "\n",
        "        # get the vectors of the input words.\n",
        "        word1_vector = self.word_vectors[word1]\n",
        "        word2_vector = self.word_vectors[word2]\n",
        "        word3_vector = self.word_vectors[word3]\n",
        "\n",
        "        # calculate the target vector.\n",
        "        target_vector = word2_vector - word1_vector + word3_vector\n",
        "        results = []\n",
        "\n",
        "\n",
        "        for word in self.word_vectors.index_to_key:\n",
        "            if word not in [word1, word2, word3]:\n",
        "                if similarity_func == 'cosine':       # calculate the similarity based on the similarity function.\n",
        "                    similarity = self.cosine_similarity(target_vector, self.word_vectors[word])\n",
        "                else:\n",
        "                    similarity = self.euclidean_similarity(target_vector, self.word_vectors[word])\n",
        "                results.append((word, similarity))\n",
        "\n",
        "        # sort the results based on the similarity score.\n",
        "        results.sort(key=lambda x: x[1], reverse=True)\n",
        "        # Returns the top num_results.\n",
        "        return results[:num_results]\n",
        "\n",
        "\n",
        "\n",
        "    def find_similar_words(self, word, num_results=5, similarity_func='cosine'):\n",
        "        \"\"\"\n",
        "        Find the most similar words to a given word.\n",
        "\n",
        "        Args:\n",
        "            word (str): Input word to find similar words for\n",
        "                Example: 'computer'\n",
        "            num_results (int): Number of similar words to return\n",
        "                Example: 5\n",
        "            similarity_func (str): Similarity function to use ('cosine' or 'euclidean')\n",
        "\n",
        "        Returns:\n",
        "            list: List of tuples (word, similarity_score) for top num_results matches\n",
        "                Example: [('laptop', 0.89), ('pc', 0.87), ('desktop', 0.85), ...]\n",
        "\n",
        "        Note:\n",
        "            - Returns empty list if input word not in vocabulary\n",
        "            - Excludes the input word from results\n",
        "            - Implementation requires iterating through entire vocabulary using:\n",
        "              for word in self.word_vectors.index_to_key\n",
        "              This exhaustive search is needed to find the most similar words\n",
        "              by comparing the target word's vector against all known words\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # if the input word is not in the vocabulary , return an empty list.\n",
        "        if word not in self.word_vectors:\n",
        "            return []\n",
        "\n",
        "        word_vector = self.word_vectors[word]\n",
        "        target_vector = word_vector\n",
        "        results = []\n",
        "\n",
        "        for word_ in self.word_vectors.index_to_key:\n",
        "            if word_ != word:  # result should not contain the input words.\n",
        "                if similarity_func == 'cosine':\n",
        "                    similarity = self.cosine_similarity(target_vector, self.word_vectors[word_])\n",
        "                else:\n",
        "                    similarity = self.euclidean_similarity(target_vector, self.word_vectors[word_])\n",
        "                results.append((word_, similarity))\n",
        "        # sort the results based on the similarity score .\n",
        "        results.sort(key=lambda x: x[1], reverse=True)\n",
        "        # return the top num_results.\n",
        "        return results[:num_results]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qll9_mcB3pVt",
        "outputId": "84f5db19-3868-4793-e04e-b0da73d4beac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading word2vec-google-news-300...\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Model downloaded successfully to: /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_embedding_ops = WordEmbeddingOps(word2vec_path)\n",
        "\n",
        "\"\"\" ## The Classic King-Man-Woman-Queen Analogy\n",
        "\n",
        " This famous analogy demonstrates how Word2Vec captures gender relationships:\n",
        " - king is to man as queen is to woman\n",
        " - Mathematically: king - man + woman ≈ queen\n",
        "\n",
        " This relationship emerged naturally during training, showing how embeddings learn semantic patterns.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def demonstrate_king_man_queen_analogy():\n",
        "    \"\"\"\n",
        "    Demonstrate the famous king:man::queen:woman analogy.\n",
        "\n",
        "    Note:\n",
        "        - Shows results using both cosine and euclidean similarity\n",
        "        - Prints intermediate vectors and calculations\n",
        "        - Useful for understanding how word analogies work\n",
        "    \"\"\"\n",
        "    print(\"Testing famous analogy: king:man::queen:?\")\n",
        "\n",
        "    # Try with cosine similarity\n",
        "    results_cos = word_embedding_ops.find_analogies(\n",
        "        \"king\", \"man\", \"queen\", similarity_func=\"cosine\")\n",
        "    print(\"\\nUsing cosine similarity:\")\n",
        "    for word, score in results_cos:\n",
        "        print(f\"  {word}: {score:.3f}\")\n",
        "\n",
        "    # Try with euclidean similarity\n",
        "    results_euc = word_embedding_ops.find_analogies(\n",
        "        \"king\", \"man\", \"queen\", similarity_func=\"euclidean\")\n",
        "    print(\"\\nUsing euclidean similarity:\")\n",
        "    for word, score in results_euc:\n",
        "        print(f\"  {word}: {score:.3f}\")\n",
        "\n",
        "demonstrate_king_man_queen_analogy()\n",
        "\n",
        "\"\"\" ## Examining Gender Bias in Word Embeddings\n",
        "\n",
        " Word embeddings can reflect and amplify societal biases present in training data. Bolukbasi et al. (2016) in \"Man is to Computer Programmer as Woman is to Homemaker?\" demonstrated systematic gender biases in word embeddings.\n",
        "\n",
        " Common problematic analogies:\n",
        " - man:doctor :: woman:nurse\n",
        " - father:businessman :: mother:housewife\n",
        "\n",
        " These biases can propagate through NLP systems, affecting downstream applications.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def demonstrate_gender_bias():\n",
        "    \"\"\"\n",
        "    Demonstrate the famous man:doctor::woman:nurse analogy.\n",
        "\n",
        "    Note:\n",
        "        - Shows results using both cosine and euclidean similarity\n",
        "        - Prints intermediate vectors and calculations\n",
        "        - Useful for understanding how word analogies work\n",
        "    \"\"\"\n",
        "\n",
        "    examples = [\n",
        "        (\"man\", \"doctor\", \"woman\"),\n",
        "        (\"father\", \"doctor\", \"mother\"),\n",
        "    ]\n",
        "\n",
        "    for word1, word2, word3 in examples:\n",
        "        print(f\"\\nTesting: {word1}:{word2}::{word3}:?\")\n",
        "\n",
        "        # Try with cosine similarity\n",
        "        results_cos = word_embedding_ops.find_analogies(\n",
        "            word1, word2, word3, similarity_func=\"cosine\")\n",
        "        print(\"\\nUsing cosine similarity:\")\n",
        "        for word, score in results_cos:\n",
        "            print(f\"  {word}: {score:.3f}\")\n",
        "\n",
        "        # Try with euclidean similarity\n",
        "        results_euc = word_embedding_ops.find_analogies(\n",
        "            word1, word2, word3, similarity_func=\"euclidean\")\n",
        "        print(\"\\nUsing euclidean similarity:\")\n",
        "        for word, score in results_euc:\n",
        "            print(f\"  {word}: {score:.3f}\")\n",
        "\n",
        "demonstrate_gender_bias()\n",
        "\n",
        "\"\"\" ## Word Similarity and Semantic Clustering\n",
        "\n",
        " Beyond analogies, word embeddings cluster semantically similar words. The example below shows example of similar word finding.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def demonstrate_similar_words():\n",
        "    \"\"\"\n",
        "    Demonstrate finding similar words for multiple example words.\n",
        "\n",
        "    Note:\n",
        "        - Tests similarity for words: cat, india, book, computer, phone\n",
        "        - Shows results using both cosine and euclidean similarity\n",
        "        - Prints top 5 similar words for each test word\n",
        "    \"\"\"\n",
        "    test_words = ['india', 'book']\n",
        "\n",
        "    for word in test_words:\n",
        "        print(f\"\\nFinding similar words for: {word}\")\n",
        "\n",
        "        # Try with cosine similarity\n",
        "        cos_similar = word_embedding_ops.find_similar_words(\n",
        "            word, similarity_func='cosine')\n",
        "        print(\"Using cosine similarity:\")\n",
        "        for similar_word, score in cos_similar:\n",
        "            print(f\"  {similar_word}: {score:.3f}\")\n",
        "\n",
        "        # Try with euclidean similarity\n",
        "        euc_similar = word_embedding_ops.find_similar_words(\n",
        "            word, similarity_func='euclidean')\n",
        "        print(\"\\nUsing euclidean similarity:\")\n",
        "        for similar_word, score in euc_similar:\n",
        "            print(f\"  {similar_word}: {score:.3f}\")\n",
        "\n",
        "demonstrate_similar_words()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFO8oxk639Ov",
        "outputId": "13e99661-230b-49d0-ac2d-cc190a9c49a8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing famous analogy: king:man::queen:?\n",
            "\n",
            "Using cosine similarity:\n",
            "  woman: 0.719\n",
            "  girl: 0.588\n",
            "  lady: 0.575\n",
            "  teenage_girl: 0.570\n",
            "  teenager: 0.538\n",
            "\n",
            "Using euclidean similarity:\n",
            "  woman: 0.303\n",
            "  girl: 0.261\n",
            "  lady: 0.258\n",
            "  teenager: 0.255\n",
            "  vivacious_blonde: 0.253\n",
            "\n",
            "Testing: man:doctor::woman:?\n",
            "\n",
            "Using cosine similarity:\n",
            "  gynecologist: 0.728\n",
            "  nurse: 0.670\n",
            "  physician: 0.667\n",
            "  doctors: 0.665\n",
            "  pediatrician: 0.640\n",
            "\n",
            "Using euclidean similarity:\n",
            "  gynecologist: 0.272\n",
            "  doctors: 0.267\n",
            "  nurse: 0.266\n",
            "  physician: 0.264\n",
            "  prenatal_checkup: 0.248\n",
            "\n",
            "Testing: father:doctor::mother:?\n",
            "\n",
            "Using cosine similarity:\n",
            "  nurse: 0.717\n",
            "  doctors: 0.680\n",
            "  physician: 0.667\n",
            "  gynecologist: 0.663\n",
            "  nurse_practitioner: 0.642\n",
            "\n",
            "Using euclidean similarity:\n",
            "  nurse: 0.287\n",
            "  doctors: 0.278\n",
            "  physician: 0.270\n",
            "  CVS_pharmacist: 0.256\n",
            "  prenatal_checkup: 0.256\n",
            "\n",
            "Finding similar words for: india\n",
            "Using cosine similarity:\n",
            "  indian: 0.697\n",
            "  usa: 0.684\n",
            "  pakistan: 0.682\n",
            "  chennai: 0.668\n",
            "  america: 0.659\n",
            "\n",
            "Using euclidean similarity:\n",
            "  indian: 0.264\n",
            "  chennai: 0.257\n",
            "  usa: 0.257\n",
            "  sri_lanka: 0.255\n",
            "  modi: 0.252\n",
            "\n",
            "Finding similar words for: book\n",
            "Using cosine similarity:\n",
            "  tome: 0.749\n",
            "  books: 0.738\n",
            "  memoir: 0.730\n",
            "  paperback_edition: 0.687\n",
            "  autobiography: 0.674\n",
            "\n",
            "Using euclidean similarity:\n",
            "  books: 0.351\n",
            "  Booklocker.com: 0.331\n",
            "  hardbound_edition: 0.329\n",
            "  Kimberla_Lawson_Roby: 0.328\n",
            "  Darin_Strauss: 0.326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BagOfWordsClassifier:\n",
        "    def __init__(self, min_freq=1):\n",
        "        \"\"\"\n",
        "        Initialize the Bag of Words classifier.\n",
        "\n",
        "        Args:\n",
        "            min_freq (int): Minimum frequency threshold for a word to be included in vocabulary.\n",
        "                           Words appearing less than min_freq times will be treated as UNK token.\n",
        "                           Default: 1 (include all words)\n",
        "\n",
        "        Attributes:\n",
        "            vocabulary (dict): Word to index mapping, including special UNK token\n",
        "                Example: {'<UNK>': 0, 'good': 1, 'movie': 2}\n",
        "            classifier: Trained logistic regression model\n",
        "            min_freq (int): Minimum frequency threshold for vocabulary inclusion\n",
        "                Example: If min_freq=2, words must appear at least twice to be included\n",
        "\n",
        "        Note:\n",
        "            - Words appearing less than min_freq times will be mapped to <UNK>\n",
        "            - <UNK> token is automatically added to vocabulary as first token (index 0)\n",
        "            - Logistic regression is used as the underlying classifier\n",
        "        \"\"\"\n",
        "        self.vocabulary = None\n",
        "        self.classifier = LogisticRegression(random_state=42)\n",
        "        self.min_freq = min_freq\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocess text by converting to lowercase, removing punctuation,\n",
        "        and filtering stop words.\n",
        "\n",
        "        Args:\n",
        "            text (str): Raw input text\n",
        "                Example: \"This movie was really good!\"\n",
        "\n",
        "        Returns:\n",
        "            list: Cleaned and tokenized words\n",
        "                Example: ['movie', 'really', 'good']\n",
        "\n",
        "        Note:\n",
        "            - Converts all text to lowercase\n",
        "            - Removes punctuation and special characters\n",
        "            - Splits text into individual tokens\n",
        "            - Removes common English stop words\n",
        "            - Stop words are removed using NLTK's English stop words list\n",
        "        \"\"\"\n",
        "        from nltk.corpus import stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        text = text.lower()\n",
        "        tokens = re.findall(r'\\w+', text)\n",
        "        return [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    def create_vocabulary(self, texts):\n",
        "        \"\"\"\n",
        "        Create vocabulary from training texts by mapping each unique word to an index,\n",
        "        considering minimum frequency threshold and adding UNK token.\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text documents\n",
        "                Example: [\n",
        "                    \"good movie good\",\n",
        "                    \"bad movie\",\n",
        "                    \"great action movie\"\n",
        "                ]\n",
        "\n",
        "        Returns:\n",
        "            dict: Word to index mapping, including UNK token\n",
        "                Example (with min_freq=2): {\n",
        "                    '<UNK>': 0,    # Special token for rare/unseen words\n",
        "                    'movie': 1,    # Frequency=3, included in vocab\n",
        "                    'good': 2,     # Frequency=2, included in vocab\n",
        "                    # 'bad', 'great', 'action' not included (frequency=1 < min_freq=2)\n",
        "                }\n",
        "\n",
        "        Note:\n",
        "            - Always includes <UNK> token at index 0\n",
        "            - Only includes words that appear >= min_freq times\n",
        "            - Word frequency is counted across all documents\n",
        "            - Uses preprocess_text function for preprocessing\n",
        "            - Words below frequency threshold will be mapped to UNK during feature extraction\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        tokenized_list = [] # list of list of tokenized preprocessed words for each text .\n",
        "        for text in texts:\n",
        "            tokens = self.preprocess_text(text)\n",
        "            tokenized_list.append(tokens)\n",
        "\n",
        "        word_freq = {} # dictionary to store frequency of each word .\n",
        "        for text in tokenized_list:\n",
        "            for token in text:\n",
        "                if token in word_freq:\n",
        "                    word_freq[token] += 1\n",
        "                else:\n",
        "                    word_freq[token] = 1\n",
        "\n",
        "        vocabulary = {'<UNK>': 0}\n",
        "        index = 1\n",
        "        for word in word_freq.keys():\n",
        "            if word_freq[word] >= self.min_freq: # if frequency of word is greater than or equal to min_freq then add it to vocabulary .\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "        return vocabulary\n",
        "\n",
        "\n",
        "    def text_to_bow(self, texts):\n",
        "        \"\"\"\n",
        "        Convert texts to bag-of-words feature vectors using the vocabulary,\n",
        "        where each element represents the count of word occurrences (not binary presence/absence).\n",
        "        Words not in vocabulary are mapped to UNK token.\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text documents\n",
        "                Example: [\"good movie good watch\", \"bad movie skip\"]\n",
        "\n",
        "        Returns:\n",
        "            np.array: Document-term matrix with UNK handling\n",
        "\n",
        "            Example: For vocabulary {'<UNK>':0, 'movie':1, 'good':2} with min_freq=2:\n",
        "            array([[1, 1, 2],    # First doc: 1 UNK ('watch'), 1 'movie', 2 'good'\n",
        "                  [2, 1, 0]])    # Second doc: 2 UNKs ('bad','skip'), 1 'movie', 0 'good'\n",
        "\n",
        "        Note:\n",
        "            - First column represents count of UNK tokens\n",
        "            - Words not in vocabulary are mapped to UNK token (index 0)\n",
        "            - Uses preprocess_text function for preprocessing\n",
        "            - Shape of output: (n_documents, len(vocabulary))\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        preprocessed_texts = [self.preprocess_text(text) for text in texts] # preprocess each text in texts .\n",
        "\n",
        "        list_ = []\n",
        "        for text in preprocessed_texts:\n",
        "            list = [0 for i in range(len(self.vocabulary))]\n",
        "            for token in text:\n",
        "                if token in self.vocabulary:\n",
        "                    list[self.vocabulary[token]] += 1# if token is in vocabulary then increment the count of that token in list .\n",
        "                else:\n",
        "                    list[0] += 1# if token is not in vocabulary then increment the count of UNK token in list .\n",
        "\n",
        "            list_.append(list)\n",
        "\n",
        "        return list_\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, X_text, y):\n",
        "        \"\"\"\n",
        "        Train the classifier on text documents.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\"good movie\", \"bad film\", \"great movie\"]\n",
        "            y (list): Class labels\n",
        "                Example: [1, 0, 1]  # 1=positive, 0=negative\n",
        "\n",
        "        Note:\n",
        "            - Creates vocabulary from training texts using min_freq threshold\n",
        "            - Converts texts to BoW features with UNK handling\n",
        "            - Trains logistic regression classifier on features\n",
        "        \"\"\"\n",
        "        # Create vocabulary from training texts\n",
        "        self.vocabulary = self.create_vocabulary(X_text)\n",
        "\n",
        "        # Convert texts to BoW features\n",
        "        X_bow = self.text_to_bow(X_text)\n",
        "\n",
        "        # Train classifier\n",
        "        self.classifier.fit(X_bow, y)\n",
        "\n",
        "    def predict(self, X_text):\n",
        "        \"\"\"\n",
        "        Predict classes for new documents.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\"amazing film\", \"terrible movie\"]\n",
        "\n",
        "        Returns:\n",
        "            list: Predicted class labels\n",
        "                Example: [1, 0]  # 1=positive, 0=negative\n",
        "\n",
        "        Note:\n",
        "            - Unknown words in test documents are mapped to UNK token\n",
        "            - Uses the same preprocessing as training\n",
        "        \"\"\"\n",
        "        # Convert texts to BoW features\n",
        "        X_bow = self.text_to_bow(X_text)\n",
        "\n",
        "        # Make predictions\n",
        "        return self.classifier.predict(X_bow)\n",
        "\n",
        "    def get_class_probabilities(self, X_text):\n",
        "        \"\"\"\n",
        "        Calculate prediction confidence scores for each class.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\"amazing film\", \"terrible movie\"]\n",
        "\n",
        "        Returns:\n",
        "            np.array: Confidence scores for each class (values 0-1)\n",
        "                Example: array([[0.1, 0.9],  # 90% confidence for positive class\n",
        "                              [0.8, 0.2]])   # 20% confidence for positive class\n",
        "\n",
        "        Note:\n",
        "            - Returns probability distribution over classes\n",
        "            - Each row sums to 1.0\n",
        "            - For binary classification:\n",
        "                - First column: confidence for negative class (0)\n",
        "                - Second column: confidence for positive class (1)\n",
        "            - Unknown words are handled via UNK token\n",
        "        \"\"\"\n",
        "        X_bow = self.text_to_bow(X_text)\n",
        "        return self.classifier.predict_proba(X_bow)\n"
      ],
      "metadata": {
        "id": "MVxJpZ3b4IwZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecClassifier:\n",
        "    def __init__(self, word2vec_path):\n",
        "        \"\"\"\n",
        "        Initialize Word2Vec classifier.\n",
        "\n",
        "        Args:\n",
        "            word2vec_path (str): Path to pre-trained word2vec model\n",
        "                Example: 'path/to/GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "        Attributes:\n",
        "            word_vectors: Loaded word vectors\n",
        "            classifier: Trained logistic regression model\n",
        "        \"\"\"\n",
        "        self.word_vectors = KeyedVectors.load_word2vec_format(\n",
        "            word2vec_path, binary=True)\n",
        "        self.classifier = LogisticRegression(random_state=42)\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocess text by converting to lowercase, removing punctuation,\n",
        "        and filtering stop words.\n",
        "\n",
        "        Args:\n",
        "            text (str): Raw input text\n",
        "                Example: \"This movie was really good!\"\n",
        "\n",
        "        Returns:\n",
        "            list: Cleaned and tokenized words\n",
        "                Example: ['movie', 'really', 'good']\n",
        "        \"\"\"\n",
        "        from nltk.corpus import stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        text = text.lower()\n",
        "        tokens = re.findall(r'\\w+', text)\n",
        "        return [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    def text_to_vec(self, texts):\n",
        "        \"\"\"\n",
        "        Convert texts to document vectors by averaging word embeddings.\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text documents\n",
        "                Example: [\"good movie\", \"bad film\"]\n",
        "\n",
        "        Returns:\n",
        "            np.array: Document vectors where each vector is average of its word vectors\n",
        "                Example shape: array([[0.2, 0.3, ..., -0.1],  # 300D vector for doc1\n",
        "                                    [0.1, 0.4, ..., -0.2]])   # 300D vector for doc2\n",
        "\n",
        "        Process:\n",
        "            1. For each document:\n",
        "                a. Split into words and preprocess\n",
        "                b. Look up word2vec vector for each word\n",
        "                c. Calculate mean of all word vectors in document\n",
        "                   - e.g., if doc has words [w1, w2, w3]:\n",
        "                     doc_vector = (vector(w1) + vector(w2) + vector(w3)) / 3\n",
        "                d. If no words found in vocabulary, vector remains zero\n",
        "\n",
        "        Note:\n",
        "            - Implementation hint: Vector size can be obtained using self.word_vectors.vector_size\n",
        "            - Each document vector has same dimensions as word vectors (e.g., 300)\n",
        "            - Words not in word2vec vocabulary are skipped\n",
        "            - Document vector is average of all found word vectors\n",
        "            - Documents with no known words get zero vectors\n",
        "            - You must use the preprocess_text function for pre-processing\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        tokenized_list = []\n",
        "        for text in texts:\n",
        "            tokens = self.preprocess_text(text)# preprocess each text in texts .\n",
        "            tokenized_list.append(tokens)\n",
        "        vectorized_list = []\n",
        "\n",
        "        # for each token in tokenized_list , calculated the vector of the token and append it to vectorized_list .\n",
        "        for tokens in tokenized_list:\n",
        "            vector = np.zeros(self.word_vectors.vector_size)\n",
        "            count = 0\n",
        "            for token in tokens:\n",
        "                if token in self.word_vectors:\n",
        "                    vector += self.word_vectors[token]\n",
        "                    count += 1\n",
        "            if count != 0:\n",
        "                vector /= count\n",
        "            vectorized_list.append(vector)\n",
        "        return np.array(vectorized_list)\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, X_text, y):\n",
        "        \"\"\"\n",
        "        Train classifier on text documents.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\"good movie\", \"bad film\", \"great movie\"]\n",
        "            y (list): Class labels\n",
        "                Example: [1, 0, 1]  # 1=positive, 0=negative\n",
        "        \"\"\"\n",
        "        # Convert texts to document vectors\n",
        "        X_vecs = self.text_to_vec(X_text)\n",
        "\n",
        "        # Train classifier\n",
        "        self.classifier.fit(X_vecs, y)\n",
        "\n",
        "    def predict(self, X_text):\n",
        "        \"\"\"\n",
        "        Predict classes for new documents.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\"amazing film\", \"terrible movie\"]\n",
        "\n",
        "        Returns:\n",
        "            list: Predicted class labels\n",
        "                Example: [1, 0]  # 1=positive, 0=negative\n",
        "        \"\"\"\n",
        "        X_vecs = self.text_to_vec(X_text)\n",
        "        return self.classifier.predict(X_vecs)\n",
        "\n",
        "    def get_class_probabilities(self, X_text):\n",
        "        \"\"\"\n",
        "        Calculate prediction confidence scores for each class.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\"amazing film\", \"terrible movie\"]\n",
        "\n",
        "        Returns:\n",
        "            np.array: Confidence scores for each class (values 0-1)\n",
        "                Example: array([[0.1, 0.9],  # 90% confidence for positive class\n",
        "                              [0.8, 0.2]])   # 20% confidence for positive class\n",
        "\n",
        "        Note:\n",
        "            - Returns probability distribution over classes\n",
        "            - Each row sums to 1.0\n",
        "            - For binary classification:\n",
        "                - First column: confidence for negative class (0)\n",
        "                - Second column: confidence for positive class (1)\n",
        "        \"\"\"\n",
        "        X_vecs = self.text_to_vec(X_text)\n",
        "        return self.classifier.predict_proba(X_vecs)\n",
        "\n"
      ],
      "metadata": {
        "id": "l2W9e_8o4WOX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Bag of Words classifier...\")\n",
        "bow_clf = BagOfWordsClassifier()\n",
        "bow_clf.fit(X_train, y_train)\n",
        "\n",
        "bow_predictions = bow_clf.predict(X_val)\n",
        "bow_accuracy = evaluate_classifier(y_val, bow_predictions)\n",
        "\n",
        "# Above 80% validation accuracy is good!\n",
        "print(f\"BoW Validation Accuracy: {bow_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i4H3mDy4gJ5",
        "outputId": "5938a3f1-1d6c-482c-e3dd-3cf090697d82"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bag of Words classifier...\n",
            "BoW Validation Accuracy: 0.8635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del bow_clf"
      ],
      "metadata": {
        "id": "AuoDlrKK-kin"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del bow_predictions"
      ],
      "metadata": {
        "id": "pnx1pOxG-qBZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTraining Word2Vec classifier...\")\n",
        "word2vec_path = download_word2vec_model()\n",
        "w2v_clf = Word2VecClassifier(word2vec_path)\n",
        "w2v_clf.fit(X_train, y_train)\n",
        "\n",
        "w2v_predictions = w2v_clf.predict(X_val)\n",
        "w2v_accuracy = evaluate_classifier(y_val, w2v_predictions)\n",
        "\n",
        "# Above 80% validation accuracy is good!\n",
        "print(f\"Word2Vec Validation Accuracy: {w2v_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgsg87qs4gvZ",
        "outputId": "72213765-33a9-4f6b-b604-30b4a03968a5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Word2Vec classifier...\n",
            "Downloading word2vec-google-news-300...\n",
            "Model downloaded successfully to: /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
            "Word2Vec Validation Accuracy: 0.8460\n"
          ]
        }
      ]
    }
  ]
}